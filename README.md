setup env
1. setup local llm (optional)
  a. install ollama from  https://ollama.com/download/mac
  b. use ollama pulll and run llm such as llama3.1, gemma2
2. python -m venv invoice-chat
3. source ./invoice-chat/bin/activate
4. python install -r requirements.txt
5. fastapi dev main.py

